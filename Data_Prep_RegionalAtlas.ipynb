{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4711a02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c47e711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41c3357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "support_data_path = \"./Datasets/Support Datasets/\"\n",
    "regionalatlas_path = \"./Datasets/Regionalatlas/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0130a187",
   "metadata": {},
   "source": [
    "### Extraktion README für Mapping Attribute Code ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57db044e",
   "metadata": {},
   "source": [
    "Zuerst werden aus der README_regionalatlas.md. Extrahiert alle Spalten der großen attribute-code-Tabelle aus der README und speichert sie als CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54a85dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attribute_table_full(md_path, csv_out):\n",
    "    \"\"\"\n",
    "    Extrahiert alle Spalten der großen attribute-code-Tabelle aus der README\n",
    "    und speichert sie als CSV.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    in_table = False\n",
    "    with open(md_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            # Tabelle beginnt mit diesem Header:\n",
    "            if line.strip().startswith(\"|attribute-title\"):\n",
    "                in_table = True\n",
    "                continue  # skip header\n",
    "            if in_table:\n",
    "                if line.strip() == \"\" or not line.strip().startswith(\"|\"):\n",
    "                    break  # Ende der Tabelle\n",
    "                parts = [c.strip() for c in line.strip().split(\"|\")[1:-1]]\n",
    "                # 6 Spalten (könnten manchmal weniger sein, daher check)\n",
    "                if len(parts) >= 6:\n",
    "                    row = {\n",
    "                        \"attribute-title\": parts[0],\n",
    "                        \"attribute-code\": parts[1],\n",
    "                        \"attribute-unit\": parts[2],\n",
    "                        \"table-title\": parts[3],\n",
    "                        \"table-code\": parts[4],\n",
    "                        \"category\": parts[5]\n",
    "                    }\n",
    "                    # Nur sinnvolle Zeilen nehmen\n",
    "                    if row[\"attribute-code\"] and row[\"attribute-title\"]:\n",
    "                        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows).drop_duplicates(\"attribute-code\")\n",
    "    df.to_csv(csv_out, index=False)\n",
    "    print(f\"Mapping geschrieben nach: {csv_out} ({len(df)} Zeilen)\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b54e278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping geschrieben nach: ./Datasets/Support Datasets/attribute_mapping2.csv (202 Zeilen)\n"
     ]
    }
   ],
   "source": [
    "# Anwendung:\n",
    "df_mapping = extract_attribute_table_full(f\"{regionalatlas_path}README_regionalatlas.md\", f\"{support_data_path}attribute_mapping.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c76768c",
   "metadata": {},
   "source": [
    "### Vorbereitung (muss nicht jedes Mal ausgeführt werden) ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317f067e",
   "metadata": {},
   "source": [
    "Zuerst müssen wir die sogenannten `table-code` der verschiedenen Tabellen des Regionalatlas identifizieren. Es gibt pro Tabelle einen `table-code`. Tabellen sind immer einer Kategorie `category` zugeordnet, es können auch mehrere Tabellen zu einer Kategorie gehören."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37af51f",
   "metadata": {},
   "source": [
    "Definition der Funktion. Hier wird die Funktion erstellt, die es erlaubt die `table-code` aus der Mapping Tabelle `attribute_mapping.csv` zu ziehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5776a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_codes_for_categories(mapping_csv_path: str, categories: list) -> dict:\n",
    "    df_map = pd.read_csv(mapping_csv_path)\n",
    "    result = {}\n",
    "    for cat in categories:\n",
    "        df_cat = df_map[df_map['category'].str.lower() == cat.lower()]\n",
    "        unique_tables = df_cat['table-code'].unique()\n",
    "        if len(unique_tables) == 0:\n",
    "            print(f\"⚠️ Keine table-codes für Kategorie '{cat}' gefunden.\")\n",
    "            result[cat] = []\n",
    "        else:\n",
    "            result[cat] = unique_tables.tolist()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4425d48b",
   "metadata": {},
   "source": [
    "Hier werden die `categories` festgelegt. Wir haben uns auf eine Auswahl relevanter Tabellen beschränkt um nicht irrelevante Daten abzufragen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9e0b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"Bildung\", \"Bevölkerung\", \"Verkehr\", \"Unternehmen\", \"Erwerbstätigkeit und Arbeitslosigkeit\", \"Landwirtschaft\", \"Industrie\", \"Bauen und Wohnen\", \"Tourismus\", \"Verkehr\", \"Öffentliche Haushalte\", \"Verdienste und Einkommen\", \"Bruttoinlandsprodukt und Bruttowertschöpfung\", \"Nachhaltigkeit\", \"Soziales\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d3a15e",
   "metadata": {},
   "source": [
    "Hier passiert die Abfrage aufgrund der zuvor festgelegten categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd3b79f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 'Bildung': table-codes: ['AI003-1', 'AI003-2', 'AI003-3']\n",
      "Category 'Bevölkerung': table-codes: ['AI002-1-5', 'AI002-2-5', 'AI002-3', 'AI002-4-5', 'AI002-5']\n",
      "Category 'Verkehr': table-codes: ['AI013-1', 'AI013-2', 'AI013-3']\n",
      "Category 'Unternehmen': table-codes: ['AI004-1', 'AI004-2', 'AI004-3']\n",
      "Category 'Erwerbstätigkeit und Arbeitslosigkeit': table-codes: ['AI007-1', 'AI007-2', 'AI008-1-5', 'AI008-2']\n",
      "Category 'Landwirtschaft': table-codes: ['AI009']\n",
      "Category 'Industrie': table-codes: ['AI010-1', 'AI010-2-5']\n",
      "Category 'Bauen und Wohnen': table-codes: ['AI011-5']\n",
      "Category 'Tourismus': table-codes: ['AI012-5']\n",
      "Category 'Öffentliche Haushalte': table-codes: ['AI015']\n",
      "Category 'Verdienste und Einkommen': table-codes: ['AI016-1', 'AI016-2-5']\n",
      "Category 'Bruttoinlandsprodukt und Bruttowertschöpfung': table-codes: ['AI017-1', 'AI017-2']\n",
      "Category 'Nachhaltigkeit': table-codes: ['AI-N-01-2-5', 'AI-N-01-3-5', 'AI-N-01-5', 'AI-N-02', 'AI-N-04', 'AI-N-05', 'AI-N-09', 'AI-N-11']\n",
      "Category 'Soziales': table-codes: ['AI-S-02', 'AI-S-03', 'AI-S-04', 'AI-S-05']\n"
     ]
    }
   ],
   "source": [
    "mapping_file = f\"{support_data_path}attribute_mapping.csv\"\n",
    "\n",
    "table_codes = get_table_codes_for_categories(mapping_file, categories)\n",
    "\n",
    "# Ausgabe\n",
    "for cat, codes in table_codes.items():\n",
    "    print(f\"Category '{cat}': table-codes: {codes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f6b49e",
   "metadata": {},
   "source": [
    "### Übersicht über Table Title, Table Code und Category ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "503c9148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alle Codes in eine Liste packen (auch falls mehrere pro Kategorie)\n",
    "all_selected_codes = [code for code_list in table_codes.values() for code in code_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d34522c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{support_data_path}attribute_mapping.csv\")\n",
    "\n",
    "# DataFrame filtern\n",
    "df_result = df[df['table-code'].isin(all_selected_codes)][['table-title', 'table-code', 'category']].drop_duplicates()\n",
    "\n",
    "# Als CSV speichern\n",
    "df_result.to_csv('selected_tables.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d57f95",
   "metadata": {},
   "source": [
    "Ergebnis `df_result` sind alle ausgewählten Tabellen `table-title` inklusive `table-code` und `category`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f30edf61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table-title</th>\n",
       "      <th>table-code</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bevölkerungsstand - Geburten - Gestorbene - Wa...</td>\n",
       "      <td>AI002-1-5</td>\n",
       "      <td>Bevölkerung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Bevölkerung nach Alter</td>\n",
       "      <td>AI002-2-5</td>\n",
       "      <td>Bevölkerung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Wanderungen nach Geschlecht und- Alter</td>\n",
       "      <td>AI002-3</td>\n",
       "      <td>Bevölkerung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Bevölkerung - Durchschnittsalter</td>\n",
       "      <td>AI002-4-5</td>\n",
       "      <td>Bevölkerung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Einbürgerungen</td>\n",
       "      <td>AI002-5</td>\n",
       "      <td>Bevölkerung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Betreute Kinder in Tagespflege/Tageseinrichtungen</td>\n",
       "      <td>AI003-1</td>\n",
       "      <td>Bildung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Schulabgänger/-innen</td>\n",
       "      <td>AI003-2</td>\n",
       "      <td>Bildung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Betreuungsquote</td>\n",
       "      <td>AI003-3</td>\n",
       "      <td>Bildung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Gewerbeanmeldungen</td>\n",
       "      <td>AI004-1</td>\n",
       "      <td>Unternehmen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Unternehmensinsolvenzen</td>\n",
       "      <td>AI004-2</td>\n",
       "      <td>Unternehmen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          table-title table-code     category\n",
       "13  Bevölkerungsstand - Geburten - Gestorbene - Wa...  AI002-1-5  Bevölkerung\n",
       "20                             Bevölkerung nach Alter  AI002-2-5  Bevölkerung\n",
       "25             Wanderungen nach Geschlecht und- Alter    AI002-3  Bevölkerung\n",
       "27                   Bevölkerung - Durchschnittsalter  AI002-4-5  Bevölkerung\n",
       "29                                     Einbürgerungen    AI002-5  Bevölkerung\n",
       "30  Betreute Kinder in Tagespflege/Tageseinrichtungen    AI003-1      Bildung\n",
       "33                               Schulabgänger/-innen    AI003-2      Bildung\n",
       "35                                    Betreuungsquote    AI003-3      Bildung\n",
       "37                                 Gewerbeanmeldungen    AI004-1  Unternehmen\n",
       "38                            Unternehmensinsolvenzen    AI004-2  Unternehmen"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e46dc5",
   "metadata": {},
   "source": [
    "## SQL Abfrage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e71d87f",
   "metadata": {},
   "source": [
    "### Vorbereitung SQL Abfrage ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff64e53d",
   "metadata": {},
   "source": [
    "Zuerst werden die Top 30 Städte in die `top30` geladen. Diese werden dann mit den jeweiligen ARS aus `plz_ars.csv` zusammengeführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4b08f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Stadt                 Gemeinde/Stadt           ARS  \\\n",
      "0         Berlin                         Berlin            11   \n",
      "1         Berlin                  Berlin, Stadt         11000   \n",
      "2         Berlin                  Berlin, Stadt  110000000000   \n",
      "3        Hamburg                        Hamburg            02   \n",
      "4        Hamburg  Hamburg, Freie und Hansestadt         02000   \n",
      "..           ...                            ...           ...   \n",
      "66  Braunschweig            Braunschweig, Stadt  031010000000   \n",
      "67          Kiel         Kiel, Landeshauptstadt         01002   \n",
      "68          Kiel         Kiel, Landeshauptstadt  010020000000   \n",
      "69      Chemnitz                Chemnitz, Stadt         14511   \n",
      "70      Chemnitz                Chemnitz, Stadt  145110000000   \n",
      "\n",
      "   Amtlicher Gemeindeschlüssel(AGS)   Textkennzeichen  \n",
      "0                               NaN               NaN  \n",
      "1                               NaN  Kreisfreie Stadt  \n",
      "2                          11000000        Stadtstaat  \n",
      "3                               NaN               NaN  \n",
      "4                               NaN  Kreisfreie Stadt  \n",
      "..                              ...               ...  \n",
      "66                         03101000  Kreisfreie Stadt  \n",
      "67                              NaN  Kreisfreie Stadt  \n",
      "68                         01002000  Kreisfreie Stadt  \n",
      "69                              NaN  Kreisfreie Stadt  \n",
      "70                         14511000  Kreisfreie Stadt  \n",
      "\n",
      "[71 rows x 5 columns]\n",
      "Anzahl unterschiedlicher Städte: 30\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "top30 = pd.read_csv(f\"{support_data_path}top30_staedte.csv\", header=None, names=['Stadt'])\n",
    "plz_ars = pd.read_csv(f\"{support_data_path}plz_ars.csv\", dtype=str)\n",
    "\n",
    "ergebnisse = []\n",
    "\n",
    "# Definiere die Ausnahmestädte, die als Teil von 'Städteregion' oder 'Region' vorkommen können\n",
    "ausnahmen = ['Aachen', 'Hannover']\n",
    "\n",
    "for stadt in top30['Stadt']:\n",
    "    if stadt in ausnahmen:\n",
    "        # Für Ausnahmen: Der Name kommt irgendwo im Text vor\n",
    "        regex = fr\".*\\b{stadt}\\b.*\"\n",
    "    else:\n",
    "        # Normalfall: Beginnt mit Stadtnamen (das deckt auch \"Leipzig, Stadt\" etc. ab)\n",
    "        regex = fr\"^{stadt}\\b\"\n",
    "    matches = plz_ars[plz_ars['Gemeinde/Stadt'].str.contains(regex, case=False, na=False, regex=True)]\n",
    "    for _, row in matches.iterrows():\n",
    "        ergebnisse.append({\n",
    "            'Stadt': stadt,\n",
    "            'Gemeinde/Stadt': row['Gemeinde/Stadt'],\n",
    "            'ARS': row['ARS'],\n",
    "            'Amtlicher Gemeindeschlüssel(AGS)': row['Amtlicher Gemeindeschlüssel(AGS)'],\n",
    "            'Textkennzeichen': row['Textkennzeichen']\n",
    "        })\n",
    "\n",
    "out = pd.DataFrame(ergebnisse)\n",
    "\n",
    "out.to_csv(f\"{support_data_path}top30_staedte_AGS_ARS.csv\", index=False)\n",
    "print(out)\n",
    "\n",
    "anzahl_staedte = out['Stadt'].nunique()\n",
    "print(f\"Anzahl unterschiedlicher Städte: {anzahl_staedte}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8141f",
   "metadata": {},
   "source": [
    "### Erstellung SQL Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7b9893",
   "metadata": {},
   "source": [
    "Nun werden mit der geraden erstellen Funktion alle relevanten SQL Queries gebaut."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d8201",
   "metadata": {},
   "source": [
    "#### Funktionen build_sql_query, normalize_table_code, fetch_regionalatlas werden gebaut ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e83e4283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def normalize_table_code(table_code: str) -> str:\n",
    "    return table_code.lower().replace('-', '_')\n",
    "\n",
    "def build_sql_query(table_code: str, ags: str, jahr: int = 2020, region_typ: int = 3) -> str:\n",
    "    tbl = normalize_table_code(table_code)\n",
    "    return (\n",
    "        f\"SELECT * \"\n",
    "        f\"FROM verwaltungsgrenzen_gesamt \"\n",
    "        f\"LEFT OUTER JOIN {tbl} ON ags = ags2 AND jahr = jahr2 \"\n",
    "        f\"WHERE typ = {region_typ} AND jahr = {jahr} AND ags = '{ags}'\"\n",
    "    )\n",
    "\n",
    "def fetch_regionalatlas(table_code: str, ags: str, jahr: int = 2020, region_typ: int = 3) -> pd.DataFrame:\n",
    "    tbl = normalize_table_code(table_code)\n",
    "    where_clause = f\"typ = {region_typ} AND jahr = {jahr} AND ags = '{ags}'\"\n",
    "    \n",
    "    layer_obj = {\n",
    "        \"source\": {\n",
    "            \"dataSource\": {\n",
    "                \"geometryType\": \"esriGeometryPolygon\",\n",
    "                \"workspaceId\": \"gdb\",\n",
    "                \"query\": build_sql_query(table_code, ags, jahr, region_typ),\n",
    "                \"oidFields\": \"id\",\n",
    "                \"spatialReference\": {\"wkid\": 25832},\n",
    "                \"type\": \"queryTable\"\n",
    "            },\n",
    "            \"type\": \"dataLayer\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    url = (\n",
    "        \"https://www.gis-idmz.nrw.de/arcgis/rest/services/\"\n",
    "        \"stba/regionalatlas/MapServer/dynamicLayer/query\"\n",
    "    )\n",
    "    params = {\n",
    "        \"layer\": json.dumps(layer_obj),\n",
    "        \"where\": where_clause,\n",
    "        \"outFields\": \"*\",\n",
    "        \"f\": \"json\"\n",
    "    }\n",
    "\n",
    "    resp = requests.get(url, params=params)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    if \"features\" in data:\n",
    "        features = [feat[\"attributes\"] for feat in data[\"features\"]]\n",
    "        return pd.DataFrame(features)\n",
    "    else:\n",
    "        raise RuntimeError(f\"Keine Daten für {table_code}, AGS {ags}: {data}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba061a2",
   "metadata": {},
   "source": [
    "##### Beispielaufruf fetch_multiple --> kann weg??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "394ea419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  typ       ags    jahr            gen   jahr2      ags2  \\\n",
      "0  457086.0  5.0     05513  2020.0  Gelsenkirchen  2020.0     05513   \n",
      "1  456989.0  5.0  05334002  2020.0         Aachen  2020.0  05334002   \n",
      "\n",
      "                                    gen2  ai0201  ai0202  ai0208  ai0209  \\\n",
      "0        Gelsenkirchen, kreisfreie Stadt  2469.1   -20.8    21.5    10.9   \n",
      "1               Aachen, kreisfreie Stadt  1547.3    -3.3    19.4     9.0   \n",
      "\n",
      "   ai0210  ai0211  ai0212       AGS  \n",
      "0    14.0   -30.8     9.9     05513  \n",
      "1    10.0   -10.0     8.7  05334002  \n"
     ]
    }
   ],
   "source": [
    "def fetch_multiple(table_code: str, ags_list: list, jahr: int = 2020, region_typ: int = 5):\n",
    "    dfs = []\n",
    "    for ags in ags_list:\n",
    "        try:\n",
    "            df_tmp = fetch_regionalatlas(table_code, ags, jahr, region_typ)\n",
    "            df_tmp[\"AGS\"] = ags\n",
    "            dfs.append(df_tmp)\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler bei {table_code}, AGS {ags}: {e}\")\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Beispiel:\n",
    "ags_list = [\"05513\", \"05334002\", \"05334000 \"]\n",
    "df_all = fetch_multiple(\"AI002-1-5\", ags_list, jahr=2020)\n",
    "print(df_all.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2daf1f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  typ       ags    jahr            gen   jahr2      ags2  \\\n",
      "0  457086.0  5.0     05513  2020.0  Gelsenkirchen  2020.0     05513   \n",
      "1  456989.0  5.0  05334002  2020.0         Aachen  2020.0  05334002   \n",
      "2  468122.0  5.0     05513  2021.0  Gelsenkirchen  2021.0     05513   \n",
      "3  468025.0  5.0  05334002  2021.0         Aachen  2021.0  05334002   \n",
      "4  479778.0  5.0     05513  2022.0  Gelsenkirchen  2022.0     05513   \n",
      "\n",
      "                                    gen2  ai0201  ai0202  ai0208  ai0209  \\\n",
      "0        Gelsenkirchen, kreisfreie Stadt  2469.1   -20.8    21.5    10.9   \n",
      "1               Aachen, kreisfreie Stadt  1547.3    -3.3    19.4     9.0   \n",
      "2        Gelsenkirchen, kreisfreie Stadt  2478.8    39.3    22.5    11.1   \n",
      "3               Aachen, kreisfreie Stadt  1548.5     7.7    20.3     8.9   \n",
      "4        Gelsenkirchen, kreisfreie Stadt  2521.9   109.9    23.3    10.7   \n",
      "\n",
      "   ai0210  ai0211  ai0212       AGS  Jahr  \n",
      "0    14.0   -30.8     9.9     05513  2020  \n",
      "1    10.0   -10.0     8.7  05334002  2020  \n",
      "2    14.0   -28.6    64.8     05513  2021  \n",
      "3     9.9   -10.0    21.0  05334002  2021  \n",
      "4    13.9   -32.5   132.4     05513  2022  \n"
     ]
    }
   ],
   "source": [
    "all_dfs = []\n",
    "for jahr in [2020, 2021, 2022, 2023, 2024, 2025]:\n",
    "    df = fetch_multiple(\"AI002-1-5\", ags_list, jahr=jahr)\n",
    "    df[\"Jahr\"] = jahr   # Damit du später weißt, zu welchem Jahr die Zeile gehört\n",
    "    all_dfs.append(df)\n",
    "\n",
    "df_gesamt = pd.concat(all_dfs, ignore_index=True)\n",
    "print(df_gesamt.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b21cbee1",
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '\nSELECT Jahr, ags, AI1501\nFROM \"AI015\"\nWHERE ags IN (?,?)\n': no such table: AI015",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tomsa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\sql.py:2664\u001b[39m, in \u001b[36mSQLiteDatabase.execute\u001b[39m\u001b[34m(self, sql, params)\u001b[39m\n\u001b[32m   2663\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2664\u001b[39m     \u001b[43mcur\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2665\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "\u001b[31mOperationalError\u001b[39m: no such table: AI015",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mDatabaseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      6\u001b[39m ags_aachen = [\u001b[33m\"\u001b[39m\u001b[33m05334000\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m05334002\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      7\u001b[39m query = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[33mSELECT Jahr, ags, AI1501\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[33mFROM \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[33mWHERE ags IN (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m.join([\u001b[33m'\u001b[39m\u001b[33m?\u001b[39m\u001b[33m'\u001b[39m]*\u001b[38;5;28mlen\u001b[39m(ags_aachen))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_sql_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mags_aachen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m conn.close()\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tomsa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\sql.py:528\u001b[39m, in \u001b[36mread_sql_query\u001b[39m\u001b[34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype, dtype_backend)\u001b[39m\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m dtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib.no_default\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tomsa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\sql.py:2728\u001b[39m, in \u001b[36mSQLiteDatabase.read_query\u001b[39m\u001b[34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[39m\n\u001b[32m   2717\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_query\u001b[39m(\n\u001b[32m   2718\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2719\u001b[39m     sql,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2726\u001b[39m     dtype_backend: DtypeBackend | Literal[\u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2727\u001b[39m ) -> DataFrame | Iterator[DataFrame]:\n\u001b[32m-> \u001b[39m\u001b[32m2728\u001b[39m     cursor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2729\u001b[39m     columns = [col_desc[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor.description]\n\u001b[32m   2731\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tomsa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\sql.py:2676\u001b[39m, in \u001b[36mSQLiteDatabase.execute\u001b[39m\u001b[34m(self, sql, params)\u001b[39m\n\u001b[32m   2673\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minner_exc\u001b[39;00m\n\u001b[32m   2675\u001b[39m ex = DatabaseError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExecution failed on sql \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2676\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mDatabaseError\u001b[39m: Execution failed on sql '\nSELECT Jahr, ags, AI1501\nFROM \"AI015\"\nWHERE ags IN (?,?)\n': no such table: AI015"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "conn = sqlite3.connect(\"regionalatlas_filtered.db\")\n",
    "table = \"AI015\"  # oder wie du den clean_title gewählt hast\n",
    "ags_aachen = [\"05334000\", \"05334002\"]\n",
    "query = f\"\"\"\n",
    "SELECT Jahr, ags, AI1501\n",
    "FROM \"{table}\"\n",
    "WHERE ags IN ({','.join(['?']*len(ags_aachen))})\n",
    "\"\"\"\n",
    "df = pd.read_sql_query(query, conn, params=ags_aachen)\n",
    "conn.close()\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de49b74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>typ</th>\n",
       "      <th>ags</th>\n",
       "      <th>jahr</th>\n",
       "      <th>gen</th>\n",
       "      <th>jahr2</th>\n",
       "      <th>ags2</th>\n",
       "      <th>gen2</th>\n",
       "      <th>ai0201</th>\n",
       "      <th>ai0202</th>\n",
       "      <th>ai0208</th>\n",
       "      <th>ai0209</th>\n",
       "      <th>ai0210</th>\n",
       "      <th>ai0211</th>\n",
       "      <th>ai0212</th>\n",
       "      <th>AGS</th>\n",
       "      <th>Jahr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>457086.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>05513</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>Gelsenkirchen</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>05513</td>\n",
       "      <td>Gelsenkirchen, kreisfreie Stadt</td>\n",
       "      <td>2469.1</td>\n",
       "      <td>-20.8</td>\n",
       "      <td>21.5</td>\n",
       "      <td>10.9</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-30.8</td>\n",
       "      <td>9.9</td>\n",
       "      <td>05513</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>456989.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>05334002</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>Aachen</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>05334002</td>\n",
       "      <td>Aachen, kreisfreie Stadt</td>\n",
       "      <td>1547.3</td>\n",
       "      <td>-3.3</td>\n",
       "      <td>19.4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>05334002</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>468122.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>05513</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>Gelsenkirchen</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>05513</td>\n",
       "      <td>Gelsenkirchen, kreisfreie Stadt</td>\n",
       "      <td>2478.8</td>\n",
       "      <td>39.3</td>\n",
       "      <td>22.5</td>\n",
       "      <td>11.1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-28.6</td>\n",
       "      <td>64.8</td>\n",
       "      <td>05513</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>468025.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>05334002</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>Aachen</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>05334002</td>\n",
       "      <td>Aachen, kreisfreie Stadt</td>\n",
       "      <td>1548.5</td>\n",
       "      <td>7.7</td>\n",
       "      <td>20.3</td>\n",
       "      <td>8.9</td>\n",
       "      <td>9.9</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>05334002</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>479778.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>05513</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>Gelsenkirchen</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>05513</td>\n",
       "      <td>Gelsenkirchen, kreisfreie Stadt</td>\n",
       "      <td>2521.9</td>\n",
       "      <td>109.9</td>\n",
       "      <td>23.3</td>\n",
       "      <td>10.7</td>\n",
       "      <td>13.9</td>\n",
       "      <td>-32.5</td>\n",
       "      <td>132.4</td>\n",
       "      <td>05513</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>479681.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>05334002</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>Aachen</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>05334002</td>\n",
       "      <td>Aachen, kreisfreie Stadt</td>\n",
       "      <td>1634.4</td>\n",
       "      <td>122.3</td>\n",
       "      <td>21.8</td>\n",
       "      <td>7.9</td>\n",
       "      <td>10.3</td>\n",
       "      <td>-24.4</td>\n",
       "      <td>142.2</td>\n",
       "      <td>05334002</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>491352.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>05513</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>Gelsenkirchen</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>05513</td>\n",
       "      <td>Gelsenkirchen, kreisfreie Stadt</td>\n",
       "      <td>2547.0</td>\n",
       "      <td>99.3</td>\n",
       "      <td>24.3</td>\n",
       "      <td>10.3</td>\n",
       "      <td>13.5</td>\n",
       "      <td>-31.8</td>\n",
       "      <td>133.0</td>\n",
       "      <td>05513</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>491255.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>05334002</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>Aachen</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>05334002</td>\n",
       "      <td>Aachen, kreisfreie Stadt</td>\n",
       "      <td>1639.9</td>\n",
       "      <td>33.6</td>\n",
       "      <td>22.6</td>\n",
       "      <td>7.5</td>\n",
       "      <td>9.8</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>52.3</td>\n",
       "      <td>05334002</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  typ       ags    jahr            gen   jahr2      ags2  \\\n",
       "0  457086.0  5.0     05513  2020.0  Gelsenkirchen  2020.0     05513   \n",
       "1  456989.0  5.0  05334002  2020.0         Aachen  2020.0  05334002   \n",
       "2  468122.0  5.0     05513  2021.0  Gelsenkirchen  2021.0     05513   \n",
       "3  468025.0  5.0  05334002  2021.0         Aachen  2021.0  05334002   \n",
       "4  479778.0  5.0     05513  2022.0  Gelsenkirchen  2022.0     05513   \n",
       "5  479681.0  5.0  05334002  2022.0         Aachen  2022.0  05334002   \n",
       "6  491352.0  5.0     05513  2023.0  Gelsenkirchen  2023.0     05513   \n",
       "7  491255.0  5.0  05334002  2023.0         Aachen  2023.0  05334002   \n",
       "\n",
       "                                    gen2  ai0201  ai0202  ai0208  ai0209  \\\n",
       "0        Gelsenkirchen, kreisfreie Stadt  2469.1   -20.8    21.5    10.9   \n",
       "1               Aachen, kreisfreie Stadt  1547.3    -3.3    19.4     9.0   \n",
       "2        Gelsenkirchen, kreisfreie Stadt  2478.8    39.3    22.5    11.1   \n",
       "3               Aachen, kreisfreie Stadt  1548.5     7.7    20.3     8.9   \n",
       "4        Gelsenkirchen, kreisfreie Stadt  2521.9   109.9    23.3    10.7   \n",
       "5               Aachen, kreisfreie Stadt  1634.4   122.3    21.8     7.9   \n",
       "6        Gelsenkirchen, kreisfreie Stadt  2547.0    99.3    24.3    10.3   \n",
       "7               Aachen, kreisfreie Stadt  1639.9    33.6    22.6     7.5   \n",
       "\n",
       "   ai0210  ai0211  ai0212       AGS  Jahr  \n",
       "0    14.0   -30.8     9.9     05513  2020  \n",
       "1    10.0   -10.0     8.7  05334002  2020  \n",
       "2    14.0   -28.6    64.8     05513  2021  \n",
       "3     9.9   -10.0    21.0  05334002  2021  \n",
       "4    13.9   -32.5   132.4     05513  2022  \n",
       "5    10.3   -24.4   142.2  05334002  2022  \n",
       "6    13.5   -31.8   133.0     05513  2023  \n",
       "7     9.8   -23.0    52.3  05334002  2023  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = df_gesamt\n",
    "df_all.head(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925e1e59",
   "metadata": {},
   "source": [
    "### Abfrage + Datenbank ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da8f438",
   "metadata": {},
   "source": [
    "In den folgenden Schritten wird eine Datenbank erstellt, in der jede zuvor ausgewählte Tabelle mit den Werten für unsere ausgewählten Gebiete befüllt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffb33a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabelle für table_code → table-title Mapping\n",
    "mapping_df = pd.read_csv(f\"{support_data_path}selected_tables.csv\")\n",
    "table_codes = mapping_df['table-code'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d88452ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11', '11000', '110000000000', '02', '02000', '020000000000', '09162', '091620000000', '09184', '05315', '053150000000', '06412', '064120000000', '05111', '051110000000', '08111', '081110000000', '14713', '147130000000', '14729', '05913', '059130000000', '04', '04011', '040110000000', '034530006006', '05113', '051130000000', '14612', '146120000000', '09564', '095640000000', '03241001', '05112', '051120000000', '05911', '059110000000', '05124', '051240000000', '05711', '057110000000', '05314', '053140000000', '08222', '082220000000', '08212', '082120000000', '08215', '05515', '055150000000', '064320015015', '073395001038', '097795725187', '09761', '097610000000', '09772', '06414', '064140000000', '05513', '055130000000', '05116', '051160000000', '05334002', '03101', '031010000000', '01002', '010020000000', '14511', '145110000000']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f\"{support_data_path}top30_staedte_AGS_ARS.csv\", dtype=str)\n",
    "\n",
    "# Definiere Sonderfälle\n",
    "sonderfaelle = ['Aachen', 'Hannover']\n",
    "\n",
    "def choose_ags(row):\n",
    "    if row['Stadt'] in sonderfaelle:\n",
    "        # Nimm Wert aus 'Amtlicher Gemeindeschlüssel(AGS)' (achten auf NaN!)\n",
    "        return row['Amtlicher Gemeindeschlüssel(AGS)']\n",
    "    else:\n",
    "        return row['ARS']\n",
    "\n",
    "# Wähle den richtigen Schlüssel je Zeile\n",
    "df['AGS_final'] = df.apply(choose_ags, axis=1)\n",
    "\n",
    "# Jetzt alle gültigen und eindeutigen AGS extrahieren\n",
    "ags_list = df['AGS_final'].dropna().unique().tolist()\n",
    "print(ags_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e78431d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gespeichert: Bevölkerungsstand_-_Geburten_-_Gestorbene_-_Wanderungen\n",
      "Gespeichert: Bevölkerung_nach_Alter\n",
      "Gespeichert: Wanderungen_nach_Geschlecht_und-_Alter\n",
      "Gespeichert: Bevölkerung_-_Durchschnittsalter\n",
      "Gespeichert: Einbürgerungen\n",
      "Gespeichert: Betreute_Kinder_in_Tagespflege_Tageseinrichtungen\n",
      "Gespeichert: Schulabgänger_-innen\n",
      "Gespeichert: Betreuungsquote\n",
      "Gespeichert: Gewerbeanmeldungen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomsa\\AppData\\Local\\Temp\\ipykernel_372\\1198911187.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  full_df = pd.concat(all_dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gespeichert: Unternehmensinsolvenzen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomsa\\AppData\\Local\\Temp\\ipykernel_372\\1198911187.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  full_df = pd.concat(all_dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gespeichert: Statistisches_Unternehmensregister:_Abhängig_Beschäftigte_AB_je_1.000_EW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomsa\\AppData\\Local\\Temp\\ipykernel_372\\1198911187.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  full_df = pd.concat(all_dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gespeichert: Erwerbstätige_ET_nach_Wirtschaftsbereichen\n",
      "Gespeichert: Beschäftigtenquote\n",
      "Gespeichert: Arbeitslosenquote,_Anteil_Arbeitslose\n",
      "Gespeichert: Arbeitslosenquote_für_ausgewählte_Personengruppen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomsa\\AppData\\Local\\Temp\\ipykernel_372\\1198911187.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  full_df = pd.concat(all_dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gespeichert: Landwirtschaftl._Betriebe_-_Viehhaltung\n",
      "Gespeichert: Investitionen\n",
      "Gespeichert: Bruttoentgelte\n",
      "Gespeichert: Bautätigkeit_und_Wohnen\n",
      "Gespeichert: Beherbergung\n",
      "Gespeichert: Pkw-Dichte\n",
      "Gespeichert: Straßenverkehrsunfälle_bezogen_auf_EW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomsa\\AppData\\Local\\Temp\\ipykernel_372\\1198911187.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  full_df = pd.concat(all_dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gespeichert: Straßenverkehrsunfälle_bezogen_auf_Kfz\n",
      "Gespeichert: Beschäftigte_im_öffentlichen_Bereich\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomsa\\AppData\\Local\\Temp\\ipykernel_372\\1198911187.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  full_df = pd.concat(all_dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gespeichert: Verfügbares_Einkommen\n",
      "Gespeichert: Einkünfte\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomsa\\AppData\\Local\\Temp\\ipykernel_372\\1198911187.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  full_df = pd.concat(all_dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gespeichert: Bruttoinlandsprodukt_BIP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomsa\\AppData\\Local\\Temp\\ipykernel_372\\1198911187.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  full_df = pd.concat(all_dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gespeichert: Bruttowertschöpfung_BWS\n",
      "Gespeichert: Flächennutzung_nach_ALKIS\n",
      "Gespeichert: Flächennutzung_nach_ALKIS_4-jährige_Veränderung\n",
      "Gespeichert: Flächennutzung_nach_ALB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomsa\\AppData\\Local\\Temp\\ipykernel_372\\1198911187.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  full_df = pd.concat(all_dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gespeichert: Landbewirtschaftung\n",
      "Gespeichert: Bevölkerung_-_Alterung\n",
      "Gespeichert: Ganztagsbetreuung_von_Kindern\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomsa\\AppData\\Local\\Temp\\ipykernel_372\\1198911187.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  full_df = pd.concat(all_dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gespeichert: Armutsgefährdung\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomsa\\AppData\\Local\\Temp\\ipykernel_372\\1198911187.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  full_df = pd.concat(all_dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gespeichert: Wirtschaftliche_Leistungsfähigkeit_-_BIP_je_Arbeitsst.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomsa\\AppData\\Local\\Temp\\ipykernel_372\\1198911187.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  full_df = pd.concat(all_dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gespeichert: Armutsgefährdung\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomsa\\AppData\\Local\\Temp\\ipykernel_372\\1198911187.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  full_df = pd.concat(all_dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gespeichert: Mindestsicherungsleistungen\n",
      "Gespeichert: Grundsicherung_für_Arbeitssuchende_SGB_II\n",
      "Gespeichert: Grundsicherung_im_Alter_und_bei_Erwerbsminderung\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Tabelle für table_code → table-title Mapping\n",
    "mapping_df = pd.read_csv(f\"{support_data_path}selected_tables.csv\")\n",
    "table_codes = mapping_df['table-code'].tolist()\n",
    "code2title = dict(zip(mapping_df['table-code'], mapping_df['table-title']))\n",
    "\n",
    "conn = sqlite3.connect('regionalatlas_filtered.db')\n",
    "\n",
    "jahre = [2020, 2021, 2022, 2023, 2024, 2025]\n",
    "\n",
    "for tbl in table_codes:\n",
    "    all_dfs = []\n",
    "    for jahr in jahre:\n",
    "        for ags in ags_list:\n",
    "            df_tmp = fetch_regionalatlas(tbl, ags, jahr, region_typ=5)\n",
    "            df_tmp[\"ags\"] = ags\n",
    "            df_tmp[\"jahr\"] = jahr\n",
    "            all_dfs.append(df_tmp)\n",
    "    # Kombiniere alles zu einem DataFrame\n",
    "    full_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    # Sauberer Tablename aus table-title\n",
    "    raw_title = code2title.get(tbl, tbl)\n",
    "    clean_title = (\n",
    "        raw_title.replace(' ', '_')\n",
    "        .replace('/', '_')\n",
    "        .replace('(', '')\n",
    "        .replace(')', '')\n",
    "    )\n",
    "    # Schreibe ALLE Jahre gemeinsam weg!\n",
    "    full_df.to_sql(clean_title, conn, if_exists=\"replace\", index=False)\n",
    "    print(f\"Gespeichert: {clean_title}\")\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6facd44f",
   "metadata": {},
   "source": [
    "#### Mapping der Attribute Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c8a1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spalten in Tabelle 'Bevölkerungsstand_-_Geburten_-_Gestorbene_-_Wanderungen' umbenannt!\n",
      "Spalten in Tabelle 'Bevölkerung_nach_Alter' umbenannt!\n",
      "Spalten in Tabelle 'Wanderungen_nach_Geschlecht_und-_Alter' umbenannt!\n",
      "Spalten in Tabelle 'Bevölkerung_-_Durchschnittsalter' umbenannt!\n",
      "Spalten in Tabelle 'Einbürgerungen' umbenannt!\n",
      "Spalten in Tabelle 'Betreute_Kinder_in_Tagespflege_Tageseinrichtungen' umbenannt!\n",
      "Spalten in Tabelle 'Schulabgänger_-innen' umbenannt!\n",
      "Spalten in Tabelle 'Betreuungsquote' umbenannt!\n",
      "Spalten in Tabelle 'Gewerbeanmeldungen' umbenannt!\n",
      "Spalten in Tabelle 'Unternehmensinsolvenzen' umbenannt!\n",
      "Spalten in Tabelle 'Statistisches_Unternehmensregister:_Abhängig_Beschäftigte_AB_je_1.000_EW' umbenannt!\n",
      "Spalten in Tabelle 'Erwerbstätige_ET_nach_Wirtschaftsbereichen' umbenannt!\n",
      "Spalten in Tabelle 'Beschäftigtenquote' umbenannt!\n",
      "Spalten in Tabelle 'Arbeitslosenquote,_Anteil_Arbeitslose' umbenannt!\n",
      "Spalten in Tabelle 'Arbeitslosenquote_für_ausgewählte_Personengruppen' umbenannt!\n",
      "Spalten in Tabelle 'Landwirtschaftl._Betriebe_-_Viehhaltung' umbenannt!\n",
      "Spalten in Tabelle 'Investitionen' umbenannt!\n",
      "Spalten in Tabelle 'Bruttoentgelte' umbenannt!\n",
      "Spalten in Tabelle 'Bautätigkeit_und_Wohnen' umbenannt!\n",
      "Spalten in Tabelle 'Beherbergung' umbenannt!\n",
      "Spalten in Tabelle 'Pkw-Dichte' umbenannt!\n",
      "Spalten in Tabelle 'Straßenverkehrsunfälle_bezogen_auf_EW' umbenannt!\n",
      "Spalten in Tabelle 'Straßenverkehrsunfälle_bezogen_auf_Kfz' umbenannt!\n",
      "Spalten in Tabelle 'Beschäftigte_im_öffentlichen_Bereich' umbenannt!\n",
      "Spalten in Tabelle 'Verfügbares_Einkommen' umbenannt!\n",
      "Spalten in Tabelle 'Einkünfte' umbenannt!\n",
      "Spalten in Tabelle 'Bruttoinlandsprodukt_BIP' umbenannt!\n",
      "Spalten in Tabelle 'Bruttowertschöpfung_BWS' umbenannt!\n",
      "Spalten in Tabelle 'Flächennutzung_nach_ALKIS' umbenannt!\n",
      "Spalten in Tabelle 'Flächennutzung_nach_ALKIS_4-jährige_Veränderung' umbenannt!\n",
      "Spalten in Tabelle 'Flächennutzung_nach_ALB' umbenannt!\n",
      "Spalten in Tabelle 'Landbewirtschaftung' umbenannt!\n",
      "Spalten in Tabelle 'Bevölkerung_-_Alterung' umbenannt!\n",
      "Spalten in Tabelle 'Ganztagsbetreuung_von_Kindern' umbenannt!\n",
      "Spalten in Tabelle 'Armutsgefährdung' umbenannt!\n",
      "Spalten in Tabelle 'Wirtschaftliche_Leistungsfähigkeit_-_BIP_je_Arbeitsst.' umbenannt!\n",
      "Spalten in Tabelle 'Armutsgefährdung' umbenannt!\n",
      "Spalten in Tabelle 'Mindestsicherungsleistungen' umbenannt!\n",
      "Spalten in Tabelle 'Grundsicherung_für_Arbeitssuchende_SGB_II' umbenannt!\n",
      "Spalten in Tabelle 'Grundsicherung_im_Alter_und_bei_Erwerbsminderung' umbenannt!\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "mapping = pd.read_csv(f\"{support_data_path}attribute_mapping.csv\")\n",
    "code2title = dict(zip(mapping['attribute-code'].str.lower(), mapping['attribute-title']))\n",
    "\n",
    "conn = sqlite3.connect('regionalatlas_filtered.db')\n",
    "\n",
    "# Hole alle relevanten Tabellennamen (z. B. aus selected_tables.csv)\n",
    "table_names = pd.read_csv(f\"{support_data_path}selected_tables.csv\")['table-title'].apply(\n",
    "    lambda x: x.replace(' ', '_').replace('/', '_').replace('(', '').replace(')', '')\n",
    ").tolist()\n",
    "\n",
    "for tablename in table_names:\n",
    "    try:\n",
    "        df = pd.read_sql_query(f'SELECT * FROM \"{tablename}\"', conn)\n",
    "        new_columns = {}\n",
    "        for col in df.columns:\n",
    "            col_lower = col.lower()\n",
    "            if col_lower in code2title:\n",
    "                new_columns[col] = code2title[col_lower]\n",
    "            else:\n",
    "                new_columns[col] = col\n",
    "        df_renamed = df.rename(columns=new_columns)\n",
    "        df_renamed.to_sql(tablename, conn, if_exists=\"replace\", index=False)\n",
    "        print(f\"Spalten in Tabelle '{tablename}' umbenannt!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Tabelle '{tablename}' konnte nicht bearbeitet werden: {e}\")\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b8771",
   "metadata": {},
   "source": [
    "#### Aufräumen der DB ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba68a4e",
   "metadata": {},
   "source": [
    "Die Spalten aus ´drop_cols´ werden aus der Datenbank entfernt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1274d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\"jahr2\", \"ags2\", \"typ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48fc4dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bevölkerungsstand_-_Geburten_-_Gestorbene_-_Wanderungen: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Bevölkerung_nach_Alter: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Wanderungen_nach_Geschlecht_und-_Alter: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Bevölkerung_-_Durchschnittsalter: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Einbürgerungen: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Betreute_Kinder_in_Tagespflege_Tageseinrichtungen: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Schulabgänger_-innen: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Betreuungsquote: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Gewerbeanmeldungen: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Unternehmensinsolvenzen: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Statistisches_Unternehmensregister:_Abhängig_Beschäftigte_AB_je_1.000_EW: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Erwerbstätige_ET_nach_Wirtschaftsbereichen: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Beschäftigtenquote: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Arbeitslosenquote,_Anteil_Arbeitslose: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Arbeitslosenquote_für_ausgewählte_Personengruppen: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Landwirtschaftl._Betriebe_-_Viehhaltung: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Investitionen: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Bruttoentgelte: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Bautätigkeit_und_Wohnen: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Beherbergung: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Pkw-Dichte: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Straßenverkehrsunfälle_bezogen_auf_EW: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Straßenverkehrsunfälle_bezogen_auf_Kfz: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Beschäftigte_im_öffentlichen_Bereich: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Verfügbares_Einkommen: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Einkünfte: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Bruttoinlandsprodukt_BIP: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Bruttowertschöpfung_BWS: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Flächennutzung_nach_ALKIS: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Flächennutzung_nach_ALKIS_4-jährige_Veränderung: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Flächennutzung_nach_ALB: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Landbewirtschaftung: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Bevölkerung_-_Alterung: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Ganztagsbetreuung_von_Kindern: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Armutsgefährdung: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Wirtschaftliche_Leistungsfähigkeit_-_BIP_je_Arbeitsst.: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Armutsgefährdung: Keine der Drop-Spalten vorhanden.\n",
      "Mindestsicherungsleistungen: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Grundsicherung_für_Arbeitssuchende_SGB_II: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n",
      "Grundsicherung_im_Alter_und_bei_Erwerbsminderung: Spalten ['jahr2', 'ags2', 'typ'] entfernt.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Liste der Tabellen, die du bearbeiten möchtest (wie bisher erzeugt)\n",
    "table_names = pd.read_csv(f\"{support_data_path}selected_tables.csv\")['table-title'].apply(\n",
    "    lambda x: x.replace(' ', '_').replace('/', '_').replace('(', '').replace(')', '')\n",
    ").tolist()\n",
    "\n",
    "conn = sqlite3.connect('regionalatlas_filtered.db')\n",
    "\n",
    "for tablename in table_names:\n",
    "    try:\n",
    "        df = pd.read_sql_query(f'SELECT * FROM \"{tablename}\"', conn)\n",
    "        # Spalten droppen, die im DataFrame vorkommen\n",
    "        drop_actual = [col for col in drop_cols if col in df.columns]\n",
    "        if drop_actual:\n",
    "            df = df.drop(columns=drop_actual)\n",
    "            df.to_sql(tablename, conn, if_exists=\"replace\", index=False)\n",
    "            print(f\"{tablename}: Spalten {drop_actual} entfernt.\")\n",
    "        else:\n",
    "            print(f\"{tablename}: Keine der Drop-Spalten vorhanden.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Tabelle '{tablename}' konnte nicht bearbeitet werden: {e}\")\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa6637e",
   "metadata": {},
   "source": [
    "### Erstellung Datenbank mit kategorisierten Tabellen ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ee2058",
   "metadata": {},
   "source": [
    "Die soeben erstellte db enthält jede einzelne Tabelle als table in der db. Wir wollen diese reduzieren, damit wir nur noch die Kategorien als Tabellen haben und in diesen Tabellen dann alle dieser Kategorie zugehörigen Tabellen vereint sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "410afde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "selected = pd.read_csv(f\"{support_data_path}selected_tables.csv\")\n",
    "# Mapping: Category → [table-title]\n",
    "category2tables = selected.groupby('category')['table-title'].apply(list).to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4346d64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neue Tabelle 'Bauen_und_Wohnen' gespeichert (grouped).\n",
      "Neue Tabelle 'Bevölkerung' gespeichert (grouped).\n",
      "Neue Tabelle 'Bildung' gespeichert (grouped).\n",
      "Neue Tabelle 'Bruttoinlandsprodukt_und_Bruttowertschöpfung' gespeichert (grouped).\n",
      "Neue Tabelle 'Erwerbstätigkeit_und_Arbeitslosigkeit' gespeichert (grouped).\n",
      "Neue Tabelle 'Industrie' gespeichert (grouped).\n",
      "Neue Tabelle 'Landwirtschaft' gespeichert (grouped).\n",
      "Neue Tabelle 'Nachhaltigkeit' gespeichert (grouped).\n",
      "Neue Tabelle 'Soziales' gespeichert (grouped).\n",
      "Neue Tabelle 'Tourismus' gespeichert (grouped).\n",
      "Neue Tabelle 'Unternehmen' gespeichert (grouped).\n",
      "Neue Tabelle 'Verdienste_und_Einkommen' gespeichert (grouped).\n",
      "Neue Tabelle 'Verkehr' gespeichert (grouped).\n",
      "Neue Tabelle 'Öffentliche_Haushalte' gespeichert (grouped).\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "# Dein bestehendes Mapping:\n",
    "# category2tables = {...} wie oben erzeugt\n",
    "\n",
    "# Neue, gruppierte DB\n",
    "grouped_db = 'regionalatlas_grouped.db'\n",
    "conn = sqlite3.connect(grouped_db)\n",
    "\n",
    "for category, table_names in category2tables.items():\n",
    "    table_names_clean = [name.replace(' ', '_').replace('/', '_').replace('(', '').replace(')', '') for name in table_names]\n",
    "    dfs = []\n",
    "    for tablename in table_names_clean:\n",
    "        try:\n",
    "            df = pd.read_sql_query(f'SELECT * FROM \\\"{tablename}\\\"', sqlite3.connect('regionalatlas_filtered.db'))\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Tabelle '{tablename}' konnte nicht geladen werden: {e}\")\n",
    "    if dfs:\n",
    "        merged = reduce(lambda left, right: pd.merge(left, right, on=['ags', 'jahr'], how='outer', suffixes=('', '_dup')), dfs)\n",
    "        merged = merged.loc[:, ~merged.columns.str.endswith('_dup')]\n",
    "        category_table = category.replace(' ', '_').replace('/', '_')\n",
    "        merged.to_sql(category_table, conn, if_exists='replace', index=False)\n",
    "        print(f\"Neue Tabelle '{category_table}' gespeichert (grouped).\")\n",
    "    else:\n",
    "        print(f\"Keine Tabellen für Category '{category}' gefunden.\")\n",
    "\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
